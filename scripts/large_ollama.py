import requests
import json
import os
import re
import time
import hashlib
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from urllib.parse import urlparse
import logging

# Python 3.8ä»¥ä¸‹ã§ã‚‚å‹•ä½œã™ã‚‹ã‚ˆã†ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from typing import Dict, List, Optional, Tuple
except ImportError:
    from typing import Dict, List, Optional
    Tuple = tuple

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """æ¤œç´¢çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    title: str
    content: str
    url: str
    source: str
    relevance_score: float = 0.0
    date: Optional[str] = None
    snippet: str = ""
    meta_info: Dict = field(default_factory=dict)

@dataclass
class AdditionalInfo:
    """è¿½åŠ æƒ…å ±ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    file_path: str
    content: str
    encoding: str = 'utf-8'
    file_size: int = 0
    last_modified: Optional[str] = None

class FileInfoManager:
    """ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ç®¡ç†ã‚¯ãƒ©ã‚¹ - æ–°æ©Ÿèƒ½"""
    
    def __init__(self, base_directory: str = "."):
        self.base_directory = base_directory
        self.supported_extensions = ['.txt', '.md', '.json', '.csv', '.log']
    
    def load_additional_info(self, file_paths: List[str]) -> List[AdditionalInfo]:
        """æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’èª­ã¿è¾¼ã¿"""
        additional_infos = []
        
        for file_path in file_paths:
            try:
                info = self._load_single_file(file_path)
                if info:
                    additional_infos.append(info)
                    print(f"âœ… è¿½åŠ æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {file_path} ({len(info.content)}æ–‡å­—)")
            except Exception as e:
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
                continue
        
        return additional_infos
    
    def _load_single_file(self, file_path: str) -> Optional[AdditionalInfo]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        
        # çµ¶å¯¾ãƒ‘ã‚¹åŒ–
        if not os.path.isabs(file_path):
            file_path = os.path.join(self.base_directory, file_path)
        
        if not os.path.exists(file_path):
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return None
        
        # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        _, ext = os.path.splitext(file_path.lower())
        if ext not in self.supported_extensions:
            print(f"âš ï¸ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {ext}")
            return None
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤å®š
        encoding = self._detect_encoding(file_path)
        
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                content = f.read()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—
            stat = os.stat(file_path)
            file_size = stat.st_size
            last_modified = datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
            
            return AdditionalInfo(
                file_path=file_path,
                content=content,
                encoding=encoding,
                file_size=file_size,
                last_modified=last_modified
            )
            
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _detect_encoding(self, file_path: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•åˆ¤å®š"""
        encodings = ['utf-8', 'shift_jis', 'euc-jp', 'cp932', 'iso-2022-jp']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    f.read(1024)  # æœ€åˆã®1KBã§åˆ¤å®š
                return encoding
            except UnicodeDecodeError:
                continue
        
        print(f"âš ï¸ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤å®šå¤±æ•—ã€UTF-8ã‚’ä½¿ç”¨: {file_path}")
        return 'utf-8'

class PromptConfigManager:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šç®¡ç†ã‚¯ãƒ©ã‚¹ - æ–°æ©Ÿèƒ½"""
    
    def __init__(self):
        self.config_file = "prompt_config.json"
        self.default_config = {
            "prompt_file": "prompt.txt",
            "additional_info_files": [],
            "enable_web_search": True,
            "file_info_priority": "high",  # high, medium, low
            "max_file_content_length": 5000,
            "file_content_summary": False
        }
    
    def load_config(self) -> Dict:
        """è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                print(f"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {self.config_file}")
                return {**self.default_config, **config}
            except Exception as e:
                print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨: {e}")
        
        return self.default_config.copy()
    
    def save_config(self, config: Dict) -> None:
        """è¨­å®šã‚’ä¿å­˜"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            print(f"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸ: {self.config_file}")
        except Exception as e:
            print(f"âŒ è¨­å®šä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def interactive_setup(self) -> Dict:
        """å¯¾è©±å‹è¨­å®š"""
        config = self.default_config.copy()
        
        print("\nğŸ“‹ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—")
        print("=" * 40)
        
        # è¿½åŠ æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
        print("\nğŸ“ è¿½åŠ æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š:")
        print("è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å…¥åŠ›ã—ã¦ãã ã•ã„")
        print("ä¾‹: data/info1.txt, data/info2.md, reference.txt")
        
        files_input = input("è¿½åŠ æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ« (ç©ºç™½ã§ã‚¹ã‚­ãƒƒãƒ—): ").strip()
        if files_input:
            file_paths = [path.strip() for path in files_input.split(',')]
            config["additional_info_files"] = file_paths
        
        # Webæ¤œç´¢ã®æœ‰åŠ¹/ç„¡åŠ¹
        web_search = input("\nWebæ¤œç´¢ã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã‹? (y/N): ").lower()
        config["enable_web_search"] = web_search == 'y'
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å„ªå…ˆåº¦
        print("\nãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®å„ªå…ˆåº¦:")
        print("  high   - ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’æœ€å„ªå…ˆ")
        print("  medium - Webæ¤œç´¢ã¨åŒç­‰")  
        print("  low    - Webæ¤œç´¢ã‚’å„ªå…ˆ")
        priority = input("å„ªå…ˆåº¦ (high/medium/low) [high]: ").lower()
        if priority in ['high', 'medium', 'low']:
            config["file_info_priority"] = priority
        
        self.save_config(config)
        return config

class SearchQueryOptimizer:
    """æ¤œç´¢ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.japanese_keywords = {
            'æœ€æ–°': 'latest',
            'ç¾åœ¨': 'current',
            'ä»Šæ—¥': 'today',
            'ã«ã¤ã„ã¦': '',
            'ã«é–¢ã—ã¦': '',
            'ã¨ã¯': 'what is',
            'ã©ã®ã‚ˆã†ã«': 'how to',
            'ãªãœ': 'why'
        }
        
        self.search_modifiers = {
            'news': 'site:news.google.com OR site:nhk.or.jp OR site:asahi.com',
            'academic': 'site:scholar.google.com OR site:researchgate.net',
            'official': 'site:go.jp OR site:gov OR site:edu',
            'recent': f'{datetime.now().year}',
            'technical': 'site:github.com OR site:stackoverflow.com'
        }
    
    def optimize_query(self, original_query: str, search_type: str = 'general') -> List[str]:
        """ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦è¤‡æ•°ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        optimized_queries = []
        
        base_query = self._clean_query(original_query)
        optimized_queries.append(base_query)
        
        english_query = self._translate_to_english(base_query)
        if english_query != base_query:
            optimized_queries.append(english_query)
        
        if search_type in self.search_modifiers:
            modified_query = f"{base_query} {self.search_modifiers[search_type]}"
            optimized_queries.append(modified_query)
        
        if any(keyword in original_query.lower() for keyword in ['æœ€æ–°', 'ç¾åœ¨', 'latest', 'current']):
            time_limited = f"{base_query} after:{datetime.now().year-1}"
            optimized_queries.append(time_limited)
        
        filtered_query = f"{base_query} -advertisement -spam"
        optimized_queries.append(filtered_query)
        
        return list(set(optimized_queries))
    
    def _clean_query(self, query: str) -> str:
        """ã‚¯ã‚¨ãƒªã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        cleaned = re.sub(r'[^\w\s\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', ' ', query)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        return cleaned
    
    def _translate_to_english(self, query: str) -> str:
        """ç°¡å˜ãªæ—¥æœ¬èª->è‹±èªç¿»è¨³"""
        translated = query
        for jp, en in self.japanese_keywords.items():
            translated = translated.replace(jp, en)
        return translated.strip()

class GoogleSearchAPI:
    """Google Search API (SerpAPI) ãƒ©ãƒƒãƒ‘ãƒ¼"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://serpapi.com/search"
        self.cache = {}
        self.cache_duration = timedelta(hours=1)
        
    def search(self, query: str, num_results: int = 10, search_type: str = 'web') -> List[SearchResult]:
        """Googleæ¤œç´¢ã‚’å®Ÿè¡Œ"""
        cache_key = hashlib.md5(f"{query}_{num_results}_{search_type}".encode()).hexdigest()
        if cache_key in self.cache:
            cached_result, timestamp = self.cache[cache_key]
            if datetime.now() - timestamp < self.cache_duration:
                logger.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ¤œç´¢çµæœã‚’å–å¾—: {query[:50]}...")
                return cached_result
        
        try:
            params = {
                'q': query,
                'api_key': self.api_key,
                'engine': 'google',
                'num': min(num_results, 20),
                'hl': 'ja',
                'gl': 'jp',
                'safe': 'active'
            }
            
            if search_type == 'news':
                params['tbm'] = 'nws'
            elif search_type == 'images':
                params['tbm'] = 'isch'
            
            logger.info(f"Googleæ¤œç´¢å®Ÿè¡Œ: {query[:50]}...")
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            results = self._parse_search_results(data, query)
            
            self.cache[cache_key] = (results, datetime.now())
            
            logger.info(f"æ¤œç´¢çµæœ {len(results)}ä»¶ã‚’å–å¾—")
            return results
            
        except Exception as e:
            logger.error(f"Googleæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _parse_search_results(self, data: dict, original_query: str) -> List[SearchResult]:
        """æ¤œç´¢çµæœã‚’ãƒ‘ãƒ¼ã‚¹"""
        results = []
        
        organic_results = data.get('organic_results', [])
        for item in organic_results:
            result = SearchResult(
                title=item.get('title', ''),
                content=item.get('snippet', ''),
                url=item.get('link', ''),
                source='Google Search',
                snippet=item.get('snippet', ''),
                meta_info={
                    'position': item.get('position', 0),
                    'domain': urlparse(item.get('link', '')).netloc
                }
            )
            
            result.relevance_score = self._calculate_relevance(result, original_query)
            results.append(result)
        
        news_results = data.get('news_results', [])
        for item in news_results:
            result = SearchResult(
                title=item.get('title', ''),
                content=item.get('snippet', ''),
                url=item.get('link', ''),
                source='Google News',
                date=item.get('date', ''),
                snippet=item.get('snippet', ''),
                meta_info={
                    'source_name': item.get('source', ''),
                    'is_news': True
                }
            )
            result.relevance_score = self._calculate_relevance(result, original_query) + 0.1
            results.append(result)
        
        results.sort(key=lambda x: x.relevance_score, reverse=True)
        return results
    
    def _calculate_relevance(self, result: SearchResult, query: str) -> float:
        """é–¢é€£åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 0.0
        query_words = set(query.lower().split())
        
        title_words = set(result.title.lower().split())
        title_matches = len(query_words.intersection(title_words))
        score += title_matches * 0.3
        
        content_words = set(result.content.lower().split())
        content_matches = len(query_words.intersection(content_words))
        score += content_matches * 0.2
        
        reliable_domains = ['wikipedia.org', 'gov.jp', 'go.jp', 'edu', 'ac.jp']
        if any(domain in result.url.lower() for domain in reliable_domains):
            score += 0.2
        
        position = result.meta_info.get('position', 10)
        score += max(0, (10 - position) * 0.05)
        
        return min(score, 1.0)

class FileBasedRAGSystem:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ RAGã‚·ã‚¹ãƒ†ãƒ  - ãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½"""
    
    def __init__(self, google_api_key: str = "", model_name: str = "gpt-oss:20b", 
                 ollama_url: str = "http://localhost:11434"):
        self.google_search = GoogleSearchAPI(google_api_key) if google_api_key else None
        self.query_optimizer = SearchQueryOptimizer()
        self.file_manager = FileInfoManager()  # æ–°æ©Ÿèƒ½
        self.prompt_config_manager = PromptConfigManager()  # æ–°æ©Ÿèƒ½
        self.model_name = model_name
        self.ollama_url = ollama_url
        
        # æ¤œç´¢ãƒˆãƒªã‚¬ãƒ¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.search_triggers = [
            'æœ€æ–°', 'ç¾åœ¨', 'ä»Š', 'ä»Šæ—¥', 'ä»Šå¹´', '2024å¹´', '2025å¹´',
            'latest', 'current', 'recent', 'today', 'now', 'update',
            'ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'news', 'å‹•å‘', 'trend', 'çŠ¶æ³', 'situation'
        ]
    
    def should_search(self, prompt: str):
        """æ¤œç´¢ãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã€æ¤œç´¢ã‚¿ã‚¤ãƒ—ã‚’è¿”ã™"""
        prompt_lower = prompt.lower()
        
        needs_search = any(trigger in prompt_lower for trigger in self.search_triggers)
        
        search_type = 'general'
        if any(word in prompt_lower for word in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'news', 'äº‹ä»¶', 'äº‹æ•…']):
            search_type = 'news'
        elif any(word in prompt_lower for word in ['ç ”ç©¶', 'study', 'è«–æ–‡', 'å­¦è¡“']):
            search_type = 'academic'
        elif any(word in prompt_lower for word in ['æ”¿åºœ', 'å…¬å¼', 'official', 'æ³•å¾‹']):
            search_type = 'official'
        elif any(word in prompt_lower for word in ['æŠ€è¡“', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒ ', 'tech', 'code']):
            search_type = 'technical'
        
        return needs_search, search_type
    
    def search_and_rank(self, query: str, search_type: str = 'general', 
                       max_results: int = 15) -> List[SearchResult]:
        """æ¤œç´¢ã‚’å®Ÿè¡Œã—ã€çµæœã‚’ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        if not self.google_search:
            return []
        
        all_results = []
        optimized_queries = self.query_optimizer.optimize_query(query, search_type)
        
        for i, opt_query in enumerate(optimized_queries[:3]):
            try:
                results = self.google_search.search(
                    opt_query, 
                    num_results=max_results // len(optimized_queries) + 3,
                    search_type=search_type
                )
                
                for result in results:
                    result.relevance_score *= (1.0 - i * 0.1)
                
                all_results.extend(results)
                
                if i < len(optimized_queries) - 1:
                    time.sleep(1)
                    
            except Exception as e:
                logger.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼ (ã‚¯ã‚¨ãƒª: {opt_query}): {e}")
                continue
        
        unique_results = self._deduplicate_results(all_results)
        unique_results.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return unique_results[:max_results]
    
    def _deduplicate_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """é‡è¤‡ã™ã‚‹æ¤œç´¢çµæœã‚’é™¤å»"""
        seen_urls = set()
        unique_results = []
        
        for result in results:
            if result.url not in seen_urls:
                seen_urls.add(result.url)
                unique_results.append(result)
        
        return unique_results
    
    def format_file_context(self, additional_infos: List[AdditionalInfo], 
                           max_length: int = 3000) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æ•´å½¢ - æ–°æ©Ÿèƒ½"""
        if not additional_infos:
            return ""
        
        context = "\n\n=== æä¾›æ¸ˆã¿è¿½åŠ æƒ…å ± ===\n"
        
        current_length = len(context)
        
        for i, info in enumerate(additional_infos, 1):
            file_header = f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ« {i}: {os.path.basename(info.file_path)}\n"
            file_header += f"   æ›´æ–°æ—¥æ™‚: {info.last_modified}\n"
            file_header += f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {info.file_size} bytes\n"
            file_header += f"   å†…å®¹:\n"
            
            # å†…å®¹ã‚’åˆ¶é™é•·ã«åˆ‡ã‚Šè©°ã‚
            remaining_length = max_length - current_length - len(file_header) - 100
            if remaining_length > 0:
                content_preview = info.content[:remaining_length]
                if len(info.content) > remaining_length:
                    content_preview += "...(çœç•¥)"
            else:
                content_preview = info.content[:200] + "...(çœç•¥)"
            
            file_text = file_header + content_preview + "\n\n"
            
            if current_length + len(file_text) > max_length:
                context += f"... (ä»– {len(additional_infos) - i + 1} ãƒ•ã‚¡ã‚¤ãƒ«çœç•¥)\n"
                break
            
            context += file_text
            current_length += len(file_text)
        
        context += "=== è¿½åŠ æƒ…å ±çµ‚äº† ===\n\n"
        return context
    
    def format_search_context(self, results: List[SearchResult], max_length: int = 2000) -> str:
        """æ¤œç´¢çµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æ•´å½¢"""
        if not results:
            return ""
        
        context = "\n\n=== Webæ¤œç´¢æƒ…å ± ===\n"
        context += f"æ¤œç´¢æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        current_length = len(context)
        
        for i, result in enumerate(results, 1):
            result_text = f"ğŸ” æ¤œç´¢çµæœ {i}: {result.title}\n"
            if result.date:
                result_text += f"   æ—¥ä»˜: {result.date}\n"
            result_text += f"   å†…å®¹: {result.content[:250]}\n"
            result_text += f"   ä¿¡é ¼åº¦: {result.relevance_score:.2f}\n"
            result_text += f"   URL: {result.url}\n\n"
            
            if current_length + len(result_text) > max_length:
                context += f"... (ä»– {len(results) - i + 1} ä»¶çœç•¥)\n"
                break
            
            context += result_text
            current_length += len(result_text)
        
        context += "=== Webæ¤œç´¢æƒ…å ±çµ‚äº† ===\n\n"
        return context
    
    def generate_response(self, prompt: str, prompt_config: Dict) -> None:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹æ¤œç´¢æ‹¡å¼µç”Ÿæˆã‚’å®Ÿè¡Œ - ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        
        print(f"\nğŸš€ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹RAGã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ")
        print("=" * 50)
        
        # 1. è¿½åŠ æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        file_context = ""
        if prompt_config.get("additional_info_files"):
            print("ğŸ“ è¿½åŠ æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            additional_infos = self.file_manager.load_additional_info(
                prompt_config["additional_info_files"]
            )
            
            if additional_infos:
                file_context = self.format_file_context(
                    additional_infos, 
                    max_length=prompt_config.get("max_file_content_length", 3000)
                )
                print(f"âœ… {len(additional_infos)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æƒ…å ±ã‚’å–å¾—")
        
        # 2. Webæ¤œç´¢å®Ÿè¡Œ
        search_context = ""
        if prompt_config.get("enable_web_search") and self.google_search:
            needs_search, search_type = self.should_search(prompt)
            
            if needs_search:
                print("ğŸ” Webæ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
                search_results = self.search_and_rank(prompt, search_type)
                
                if search_results:
                    search_context = self.format_search_context(search_results)
                    print(f"âœ… {len(search_results)}ä»¶ã®Webæ¤œç´¢çµæœã‚’å–å¾—")
        
        # 3. æƒ…å ±çµ±åˆï¼ˆå„ªå…ˆåº¦ã«åŸºã¥ã„ã¦é †åºæ±ºå®šï¼‰
        priority = prompt_config.get("file_info_priority", "high")
        
        if priority == "high":
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å„ªå…ˆ
            combined_context = file_context + search_context
        elif priority == "low": 
            # Webæ¤œç´¢ã‚’å„ªå…ˆ
            combined_context = search_context + file_context
        else:
            # medium: ãƒãƒ©ãƒ³ã‚¹è‰¯ã
            combined_context = file_context + search_context
        
        # 4. æœ€çµ‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        final_prompt = self._build_final_prompt(prompt, combined_context, prompt_config)
        
        print(f"\nğŸ¯ æ§‹ç¯‰ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(final_prompt)}æ–‡å­—")
        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±: {len(file_context)}æ–‡å­—")
        print(f"ğŸ” æ¤œç´¢æƒ…å ±: {len(search_context)}æ–‡å­—")
        
        # 5. LLMå¿œç­”ç”Ÿæˆ
        print(f"\nğŸ¤– {self.model_name} ã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆ...")
        print("=" * 50)
        
        self._generate_llm_response(final_prompt)
    
    def _build_final_prompt(self, original_prompt: str, context: str, config: Dict) -> str:
        """æœ€çµ‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        
        if not context:
            return original_prompt
        
        final_prompt = f"""{context}

ä¸Šè¨˜ã®æƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ã€ä»¥ä¸‹ã®è³ªå•ã«è©³ã—ãå›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€è³ªå•ã€‘
{original_prompt}

ã€å›ç­”æŒ‡ç¤ºã€‘
- æä¾›ã•ã‚ŒãŸè¿½åŠ æƒ…å ±ã¨Webæ¤œç´¢çµæœã‚’æ´»ç”¨ã—ã¦ãã ã•ã„
- å…·ä½“çš„ã§å®Ÿç”¨çš„ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„
- æƒ…å ±æºãŒã‚ã‚‹å ´åˆã¯é©åˆ‡ã«å‚ç…§ã—ã¦ãã ã•ã„
- å›ç­”ã‚’é€”ä¸­ã§æ­¢ã‚ãšã€å®Œå…¨ãªå†…å®¹ã‚’æä¾›ã—ã¦ãã ã•ã„
- æœ€å¾Œã«è¦ç‚¹ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„

ã€å›ç­”ã€‘"""
        
        return final_prompt
    
    def _generate_llm_response(self, prompt: str) -> None:
        """LLMå¿œç­”ç”Ÿæˆ"""
        url = f"{self.ollama_url}/api/generate"
        
        data = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": True,
            "options": {
                "temperature": 0.3,
                "top_p": 0.9,
                "max_tokens": 4000,
                "repeat_penalty": 1.1,
                "stop": []
            }
        }
        
        try:
            response = requests.post(url, json=data, stream=True, timeout=120)
            response.raise_for_status()
            
            print("ğŸ“ å›ç­”:")
            
            for line in response.iter_lines():
                if line:
                    try:
                        decoded_line = line.decode('utf-8')
                        response_data = json.loads(decoded_line)
                        
                        if 'response' in response_data and response_data['response']:
                            print(response_data['response'], end='', flush=True)
                        
                        if response_data.get('done', False):
                            break
                            
                    except json.JSONDecodeError:
                        continue
            
            print("\n")
            
        except Exception as e:
            logger.error(f"å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # åŸºæœ¬è¨­å®šèª­ã¿è¾¼ã¿
    config_file = "config.json"
    
    if os.path.exists(config_file):
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
    else:
        config = {
            "google_api_key": "",
            "model_name": "gpt-oss:20b", 
            "ollama_url": "http://localhost:11434"
        }
        
        print("âš™ï¸ åŸºæœ¬è¨­å®šã‚’è¡Œã„ã¾ã™")
        config["google_api_key"] = input("SerpAPI ã‚­ãƒ¼ (ç©ºç™½ã§Webæ¤œç´¢ç„¡åŠ¹): ")
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"âœ… åŸºæœ¬è¨­å®šã‚’ {config_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šç®¡ç†
    prompt_config_manager = PromptConfigManager()
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã®èª­ã¿è¾¼ã¿ã¾ãŸã¯ä½œæˆ
    if not os.path.exists(prompt_config_manager.config_file):
        print("\nğŸ“‹ åˆå›ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’è¡Œã„ã¾ã™")
        prompt_config = prompt_config_manager.interactive_setup()
    else:
        prompt_config = prompt_config_manager.load_config()
        
        # è¨­å®šå¤‰æ›´ã®ç¢ºèª
        change_config = input("\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã‚’å¤‰æ›´ã—ã¾ã™ã‹? (y/N): ").lower()
        if change_config == 'y':
            prompt_config = prompt_config_manager.interactive_setup()
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿
    prompt_file = prompt_config.get("prompt_file", "prompt.txt")
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        with open(os.path.join(script_dir, prompt_file), 'r', encoding='utf-8') as f:
            prompt_text = f.read().strip()
    except FileNotFoundError:
        print(f"âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{prompt_file}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    rag_system = FileBasedRAGSystem(
        google_api_key=config.get("google_api_key", ""),
        model_name=config.get("model_name", "gpt-oss:20b"),
        ollama_url=config.get("ollama_url", "http://localhost:11434")
    )
    
    # è¨­å®šæƒ…å ±è¡¨ç¤º
    print("\nğŸš€ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹RAGã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {prompt_file}")
    print(f"ğŸ“ è¿½åŠ æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«: {len(prompt_config.get('additional_info_files', []))}å€‹")
    if prompt_config.get('additional_info_files'):
        for i, file_path in enumerate(prompt_config['additional_info_files'], 1):
            print(f"    {i}. {file_path}")
    print(f"ğŸ” Webæ¤œç´¢: {'æœ‰åŠ¹' if prompt_config.get('enable_web_search') else 'ç„¡åŠ¹'}")
    print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«å„ªå…ˆåº¦: {prompt_config.get('file_info_priority', 'high')}")
    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {config.get('model_name', 'gpt-oss:20b')}")
    print("=" * 60)
    
    # å¿œç­”ç”Ÿæˆå®Ÿè¡Œ
    rag_system.generate_response(prompt_text, prompt_config)

if __name__ == "__main__":
    main()